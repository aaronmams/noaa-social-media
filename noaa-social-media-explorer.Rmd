---
title: "NOAA Social Media Explorer"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

As kind of a fun thing to do I started tracking Twitter activity related to a number of NOAA accounts.  I've been storing this information in a relational database that I refresh about once a week to get new activity.  I've also created a basic R Shiny App to provide a way for others to explore these data.

I thought this might be a cool thing for the RUsers to talk about since it contains elements of three R functionalities that people might be interested in:

1. R packages for accessing public data through APIs
2. The RODBC package for working with relational databases in R
3. The use of R Shiny to allow others to explore your data

## Outline

The project follows a multi-step process to display information about NOAA-related activity on Twitter.  This document is organized to provide detail on each step:

### Step 0: Set up a database

For this project I set up two tables inside a database called ECON on our lab's Pinniger server. This initial set up was done entirely with MS SQL Server Management Studio, involved no R-ing, was really simple, and further discussion is likely of no interest to anyone in the group.  Moving on.

### Step 1: Collect and organize Twitter activity from NOAA accounts

This step uses the [rtweet](http://rtweet.info/) package to manage calls to Twitter's API.  

### Step 2: Push the Twitter data up to the database

This step uses the [RODBC Package](https://cran.r-project.org/web/packages/RODBC/index.html) to connect to my databases on Pinniger and add Twitter data to existing tables. 

### Step 3: Save the updated Twitter data to local .csv files

This step is clunky and probably unnecessary in the long run...but for now I don't really want my Shiny app connecting to SQL Server databases on Pinniger so I'm saving the data locally as well as in database tables.

### Step 4: Create an R Shiny App to display the data

Using [Shiny](https://shiny.rstudio.com/) I created an interactive explorer for the NOAA Twitter data that allows users to choose a subset of NOAA accounts and a time window and outputs:

* most retweeted hashtags over the time window
* temporal evolution of hashtag popularity
* most retweeted accounts
* evolution of followers by account over the time window
* the text of the most retweeted tweets over the chosen time window


## Step 1: Collect the tweets

### Step 1A: Set up communication with the Twitter API

In order to use the [rtweet](http://rtweet.info/) package to send requests to Twitter's API you have to register an application with Twitter.  This is not difficult to do but is a little tedious to explain.

Basically, you have to go to [Twitter's Developers page](https://apps.twitter.com) and register an app with them.  When you do that, you're app will be assigned access keys that can be used to set up the connection to Twitter's API.

[Michael Kearney](http://rtweet.info/articles/auth.html) has a pretty thorough guide on setting up the rtweet package.

```{r,message=FALSE}

library(data.table)
library(dplyr)
library(ggplot2)
library(rtweet)
library(RODBC)

#------------------------------------------------------------------
#Twitter credentials

#pw: mypassword
#username: aaronmams
#email: aaron.mams@gmail.com
#app name: mamultron
#owner: mamulsaurus
#owner id: myownerid
#------------------------------------------------------------------

# Uncomment everything below here for setup
#consumer_key <- ''
#consumer_secret <- ''
#access_token <- ''
#access_secret <- ''

#twitter_token <- create_token(app='mamultron',
#                              consumer_key=consumer_key,
#                              consumer_secret=consumer_secret)
#
```


I've commented out the setup code above because the setup only needs to be done once.  This is a fun improvement over the twittR package which required authentication for every new R session.

### Step 1B: Pull some tweets

Next, I define a list of NOAA Twitter accounts that I want to pull data for:
```{r}
users <- c('@NOAAHabitat','@NOAAFisheries','@NOAAFish_WCRO','@NOAA','@NOAAResearch','@NOAANCEIclimate',
           '@NOAAClimate','@MBNMS','@NOAA_CINMS','@Eileen_NOAAFish','@NOAAFisheriesAK','@NOAAFish_PIRO',
           '@NOAAFish_PIFSC','@NOAAFish_SERO','@CenterForBioDiv')
```


The data pull happens in two parts:

1. use the get_timeline() method to get all the tweets sent from the various NOAA accounts
2. use the search_tweets() to get tweets mentioning any of the NOAA accounts in the list above

```{r}
# get tweets from the NOAA accounts
#tl <- get_timeline('@NOAAHabitat',n=10,retryonratelimit=TRUE)

#tl <- tl %>% select(created_at,user_id,screen_name,text,
#                    is_quote,is_retweet,favorite_count,
#                    retweet_count,hashtags,mentions_screen_name) %>%
#  mutate(update_time=Sys.time())

#str(tl)
#print.data.frame(tl[1:3,])



# get tweets mentioning any NOAA account
#  rt <- search_tweets(
#    '@NOAAHabitat', n = 10, include_rts = FALSE
#  )
#  rt <- rt %>% select(created_at,user_id,screen_name,text,
#                      is_quote,is_retweet,favorite_count,
#                      retweet_count,hashtags,mentions_screen_name) %>% 
#    mutate(update_time=Sys.time())
  
#str(rt)
#print.data.frame(rt[1:3,])
```

For illustrative purposes look at the list of accounts from each data frame above:

```{r}
#unique(tl$screen_name)
#unique(rt$screen_name)
```

### Step 1C: 